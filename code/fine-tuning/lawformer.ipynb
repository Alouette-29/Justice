{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at thunlp/Lawformer were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at thunlp/Lawformer and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "lawformer = AutoModel.from_pretrained(\"thunlp/Lawformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "class CailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path \n",
    "        file = open(path,'r',encoding='utf-8')\n",
    "        cases = file.readlines()\n",
    "        self.cases = [json.loads(i) for i in cases]\n",
    "        self.length = len(self.cases)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.cases[index]\n",
    "        caseA = \"\".join(data['Case_A'])\n",
    "        caseB = \"\".join(data['Case_B'])\n",
    "        label = data['label']\n",
    "        inputs = tokenizer([caseA,caseB], return_tensors=\"pt\",padding=\"max_length\",max_length=300,truncation=True)\n",
    "        return inputs,label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "without collate \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([[ 101, 5307, 2144,  ..., 6158, 1440,  102],\n",
       "          [ 101, 5307, 2144,  ..., 4638, 1297,  102],\n",
       "          [ 101, 5307, 2144,  ..., 1440,  782,  102],\n",
       "          [ 101, 1062, 6401,  ...,  809, 2807,  102],\n",
       "          [ 101, 5307, 2144,  ..., 8790, 5439,  102],\n",
       "          [ 101, 5307, 2144,  ..., 6395,  816,  102]]),\n",
       "  'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0],\n",
       "          [0, 0, 0,  ..., 0, 0, 0]]),\n",
       "  'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1],\n",
       "          [1, 1, 1,  ..., 1, 1, 1]])},\n",
       " [0, 1, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = CailDataset('./cail/competition_stage_2_train.json')\n",
    "def collate_fn(data):\n",
    "    tensors = [i[0] for i in data]\n",
    "    labels = [i[1]  for i in data]\n",
    "    inputs_ids = torch.cat([i['input_ids'] for i in tensors])\n",
    "    token_type_ids = torch.cat([i['token_type_ids'] for i in tensors])\n",
    "    mask = torch.cat([i['attention_mask'] for i in tensors])\n",
    "    inputs = {\"input_ids\":inputs_ids,\"token_type_ids\":token_type_ids,\"attention_mask\":mask}\n",
    "    return inputs,labels\n",
    "#数据加载器\n",
    "loader = torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                     batch_size=3,\n",
    "                                     collate_fn=collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "# 查看数据样例\n",
    "for i, batch in enumerate(loader):\n",
    "    break\n",
    "\n",
    "#aloader = torch.utils.data.DataLoader(dataset=dataset,batch_size=3,shuffle=False,drop_last=True)\n",
    "# for i,abatch in enumerate(aloader):\n",
    "#     break\n",
    "def manifest(name,batch):\n",
    "    print(name)\n",
    "    print(type(batch))# list \n",
    "    print(len(batch))# ==batch_size\n",
    "    print(type(batch[0])) # tuple = dictionary of data + label\n",
    "    print(type(batch[0][0]),type(batch[0][1])) # dict and int \n",
    "    for i in batch[0][0]:\n",
    "        print(i,batch[0][0][i].shape)\n",
    "    print(\"end\")\n",
    "\n",
    "# manifest(\"with collate_fn\",batch)\n",
    "#manifest(\"without collate_fn\",abatch)\n",
    "print(\"without collate \")\n",
    "# print(type(abatch),len(abatch))\n",
    "# print(type(abatch[0]),type(abatch[1]))\n",
    "# for key in abatch[0]:\n",
    "#     print(key,abatch[0][key].shape)\n",
    "# print(abatch[1])\n",
    "# print(len(loader))\n",
    "# print(tokenizer.decode(inputs['input_ids'][0][1]))\n",
    "# # print(labels[0])\n",
    "#print(len(batch),len(batch[0]),len(batch[0][0]))\n",
    "# for k, v in inputs.items():\n",
    "#     print(k, v.shape)\n",
    "# inputs, labels = dataset[0]\n",
    "\n",
    "# len(dataset), inputs['input_ids'].shape, labels\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "time1 = time()\n",
    "for b in batch:\n",
    "    # print(type(b[0]))\n",
    "    # for key in b[0]:\n",
    "    #     print(key,b[0][key].shape)\n",
    "    lawformer(**b[0])\n",
    "time2 = time()\n",
    "\n",
    "# print(tensorcopy.shape)\n",
    "# print(tensorcopy2.shape)\n",
    "# print(tensorcopy3.shape)\n",
    "time3 = time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.718900918960571 10.254740953445435\n"
     ]
    }
   ],
   "source": [
    "print(time2-time1,time3-time2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = CailDataset('./cail/competition_stage_2_train.json')\n",
    "testset = CailDataset('./cail/competition_stage_2_test.json')\n",
    "#数据加载器\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "testloader = torch.utils.data.DataLoader(dataset=testset,\n",
    "                                     batch_size=16,\n",
    "                                     shuffle=False,\n",
    "                                     drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#模型试算\n",
    "#[b, lens] -> [b, lens, 768]\n",
    "#pretrained(**inputs).last_hidden_state.shape\n",
    "import torch  \n",
    "class Lawfomer2Super(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = lawformer\n",
    "        # 300 is max length of lawformer \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=300,out_channels=1,kernel_size=1)\n",
    "        self.conv2d  = torch.nn.Conv2d(in_channels = 1, out_channels = 2, kernel_size=(2,1),stride=(2,1))\n",
    "        self.fc = torch.nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.keys())\n",
    "        # inputs : batch_size * max_length * 768 \n",
    "        # conv1d output : batch_size * 1 * 768\n",
    "        # result : batch_size*1 * 3 \n",
    "        if self.tuneing:\n",
    "            #print(\"here\")\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.pretrained(**inputs).last_hidden_state\n",
    "        out = self.conv1d(out)\n",
    "        out = self.conv2d(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in self.pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            lawformer.train()\n",
    "            self.pretrained = lawformer\n",
    "        else:\n",
    "            for i in lawformer.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            lawformer.eval()\n",
    "            print(\"any\")\n",
    "            self.pretrained = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf2classify = Lawfomer2Super()\n",
    "lf2classify.fine_tuneing(tuneing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 300, 768])\n",
      "torch.Size([6, 1, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 3])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#inputs = {\"input_ids\":tensorcopy[:4],\"token_type_ids\":tensorcopy2[:4],\"attention_mask\":tensorcopy3[:4]}\n",
    "lf2classify(batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,label =trainset[0][0],trainset[0][1]\n",
    "for i in data:\n",
    "    print(i,data[i].shape)\n",
    "lf2classify(data)\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "for i , (data1,label1) in enumerate(trainloader):\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data:\n",
    "    print(i,data1[i].shape,data[i].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "#训练\n",
    "from tqdm import tqdm,trange\n",
    "def train(epochs):\n",
    "    lr = 1e-3 if lf2classify.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(lf2classify.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    lf2classify.train()\n",
    "    for epoch in range(epochs):\n",
    "        lf2classify.train()\n",
    "        step = 0 \n",
    "        #for step, (inputs, labels) in tqdm(enumerate(trainloader)):\n",
    "        for i in trange(len(trainset.cases)):\n",
    "            (inputs, labels) = trainset[i]\n",
    "            step+=1\n",
    "            #模型计算\n",
    "            #print(len(inputs),inputs.keys())\n",
    "            outs = lf2classify(inputs)\n",
    "            #梯度下降\n",
    "            outs = outs.squeeze()\n",
    "            #print(outs,labels)\n",
    "            one_hot = torch.zeros(3)\n",
    "            one_hot[labels] = 1\n",
    "            loss = criterion(outs, torch.FloatTensor(one_hot))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if step % 50 == 0:\n",
    "                print(\"in epoch \"+str(epoch)+\" current train loss is \"+str(loss.item()))\n",
    "        if epoch%10==0 or epoch==epochs-1:\n",
    "            lf2classify.eval()\n",
    "            correct = 0 \n",
    "            with torch.no_grad:\n",
    "                for step, (inputs, labels) in enumerate(testloader):\n",
    "                    outs = lf2classify(inputs)\n",
    "                    fakes = torch.argmax(outs)\n",
    "                    correct+=sum(fakes==labels)\n",
    "            \n",
    "            print(\"in epoch \"+str(epoch)+\" current validation accuracy is \"+str(correct/len(testloader)))\n",
    "\n",
    "        torch.save(lf2classify, 'model/lawformer_微调.model')\n",
    "\n",
    "\n",
    "print(sum(p.numel() for p in lf2classify.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "criterion(torch.FloatTensor(3),torch.zeros(3))\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lawformer(**inputs).last_hidden_state.shape\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in inputs:\n",
    "    print(inputs[i].shape)\n",
    "lf2classify(inputs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63aff658242b17a645c83dadf2961dc7d0eea54a0eef17e6cd4d269f5b28227b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
