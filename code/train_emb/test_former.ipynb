{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## how to build sentence level embedding model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, losses, util, InputExample,models\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "\"\"\"\n",
    "要进行句子级别的文本嵌入，\n",
    "必要用一个词嵌入模型来提取文本特征\n",
    "然后用一个池化层来对其向量\n",
    "然后用一个线性层来任意调节向量长度  \n",
    "\"\"\"\n",
    "# 定义 词嵌入的模型\n",
    "word_embedding_model = models.Transformer('bert-base-chinese', max_seq_length=256)\n",
    "# 定义池化层\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "# 最后组合模型 \n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "path: /cail/sentence_relation_set.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6123]])\n"
     ]
    }
   ],
   "source": [
    "# 模型前向传播实例 \n",
    "example = ['银行借贷','套路贷']\n",
    "example_tensor  =  model.encode(example)\n",
    "example_tensor.shape\n",
    "print(util.cos_sim(example_tensor[0],example_tensor[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设计下游的文本相似度任务\n",
    "path = \"../cail/sentence_relation_set.json\"\n",
    "datafile = open(path,'r',encoding='utf-8')\n",
    "import json\n",
    "train_set  = []\n",
    "for i in range(4):\n",
    "    line = datafile.readline()\n",
    "    line = json.loads(line)\n",
    "    train_set.append(line)\n",
    "train_set\n",
    "dev_samples = train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from datetime import datetime  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class downstramset(Dataset):\n",
    "    def __init__(self,data) -> None:\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "    def __getitem__(self, index) :\n",
    "        item =self.data[index]\n",
    "        s1 = item['s1']\n",
    "        s2 = item['s2']\n",
    "        score = float(item['label'])\n",
    "        \n",
    "        return InputExample(texts=[s1,s2], label=score)\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1\n",
    "model_name = 'law_bert'\n",
    "model_save_path = 'output/-'+model_name+'-'+datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "trainset = downstramset(train_set)\n",
    "train_dataloader = DataLoader(trainset,shuffle=True)\n",
    "train_loss  = losses.CosineSimilarityLoss(model=model)\n",
    "def dict2input(item):\n",
    "    s1 = item['s1']\n",
    "    s2 = item['s2']\n",
    "    score = str(item['label'])\n",
    "    return  InputExample(texts=[s1,s2], label=score)\n",
    "dev_samples = [dict2input(i) for  i in train_set]\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(dev_samples, name='sts-dev')\n",
    "\n",
    "\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs * 0.1) #10% of train data for warm-up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s1': '上海市长宁区人民检察院指控:2020年3月,被告人XXX受他人指使在广东省揭阳市使用自己的身份证办理中国农业银行卡、手机卡及网银U盾1套,并以人民币1,000元的价格出售给他人。',\n",
       "  's2': '桂林市象山区人民检察院指控,2019年12月,XXX、XXX(二人均另案处理)、XXX(在逃)为获取非法利益,合谋组织搭建“亿万家”平台为境外赌博等非法网站提供支付结算服务收取佣金。',\n",
       "  'label': '1'},\n",
       " {'s1': '桂林市象山区人民检察院指控,2019年12月,XXX、XXX(二人均另案处理)、XXX(在逃)为获取非法利益,合谋组织搭建“亿万家”平台为境外赌博等非法网站提供支付结算服务收取佣金。',\n",
       "  's2': '上海市长宁区人民检察院指控:2020年3月,被告人XXX受他人指使在广东省揭阳市使用自己的身份证办理中国农业银行卡、手机卡及网银U盾1套,并以人民币1,000元的价格出售给他人。',\n",
       "  'label': '1'},\n",
       " {'s1': '该农业银行卡支付结算金额超人民币1,860余万元。',\n",
       "  's2': '截止2020年6月12日,XXX为“亿万家”平台提供转账等支付结算服务,资金结算金额达565884元,非法获利5000元。',\n",
       "  'label': '1'},\n",
       " {'s1': '截止2020年6月12日,XXX为“亿万家”平台提供转账等支付结算服务,资金结算金额达565884元,非法获利5000元。',\n",
       "  's2': '该农业银行卡支付结算金额超人民币1,860余万元。',\n",
       "  'label': '1'}]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 4/4 [00:00<00:00,  5.82it/s]\n",
      "d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4427: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(msg))\n",
      "d:\\programs\\anaconda\\envs\\pytorch\\lib\\site-packages\\scipy\\stats\\_stats_py.py:4881: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  warnings.warn(stats.ConstantInputWarning(warn_msg))\n",
      "Epoch: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s]\n"
     ]
    }
   ],
   "source": [
    "model.fit(train_objectives=[(train_dataloader, train_loss)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "#\n",
    "# Load the stored model and evaluate its performance on STS benchmark dataset\n",
    "#\n",
    "##############################################################################\n",
    "\n",
    "model = SentenceTransformer(model_save_path)\n",
    "test_evaluator = EmbeddingSimilarityEvaluator.from_input_examples(test_samples, name='sts-test')\n",
    "test_evaluator(model, output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)7023f/.gitattributes: 100%|██████████| 736/736 [00:00<00:00, 737kB/s]\n",
      "Downloading (…)_Pooling/config.json: 100%|██████████| 190/190 [00:00<00:00, 108kB/s]\n",
      "Downloading (…)433037023f/README.md: 100%|██████████| 3.71k/3.71k [00:00<00:00, 3.74MB/s]\n",
      "Downloading (…)3037023f/config.json: 100%|██████████| 679/679 [00:00<?, ?B/s] \n",
      "Downloading (…)ce_transformers.json: 100%|██████████| 122/122 [00:00<?, ?B/s] \n",
      "Downloading (…)33037023f/merges.txt: 100%|██████████| 456k/456k [00:01<00:00, 394kB/s]\n",
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 329M/329M [00:36<00:00, 9.00MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 52.0/52.0 [00:00<00:00, 26.8kB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 239/239 [00:00<00:00, 120kB/s]\n",
      "Downloading (…)7023f/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:01<00:00, 955kB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 1.12k/1.12k [00:00<00:00, 1.12MB/s]\n",
      "Downloading (…)33037023f/vocab.json: 100%|██████████| 798k/798k [00:01<00:00, 589kB/s]\n",
      "Downloading (…)037023f/modules.json: 100%|██████████| 229/229 [00:00<00:00, 115kB/s]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "\n",
    "model = SentenceTransformer('nli-distilroberta-base-v2')\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "train_dataset = SentencesDataset(train_examples, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from torch import nn\n",
    "\n",
    "### \n",
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=256)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension())\n",
    "dense_model = models.Dense(in_features=pooling_model.get_sentence_embedding_dimension(), out_features=256, activation_function=nn.Tanh())\n",
    "\n",
    "model = SentenceTransformer(modules=[word_embedding_model, pooling_model, dense_model])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###\n",
    "from sentence_transformers import SentenceTransformer, InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "   InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#Define the model. Either from scratch of by loading a pre-trained model\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "#Define your train examples. You need more than just two examples...\n",
    "train_examples = [InputExample(texts=['My first sentence', 'My second sentence'], label=0.8),\n",
    "    InputExample(texts=['Another pair', 'Unrelated sentence'], label=0.3)]\n",
    "\n",
    "#Define your train dataset, the dataloader and the train loss\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "#Tune the model\n",
    "model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1, warmup_steps=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63aff658242b17a645c83dadf2961dc7d0eea54a0eef17e6cd4d269f5b28227b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
