{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b15c1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at DPR and are newly initialized: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='DPR', vocab_size=23283, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids torch.Size([2, 37])\n",
      "token_type_ids torch.Size([2, 37])\n",
      "attention_mask torch.Size([2, 37])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 768])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "#加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained('DPR')\n",
    "from transformers import AutoModel\n",
    "\n",
    "#加载预训练模型\n",
    "pretrained = AutoModel.from_pretrained('DPR')\n",
    "print(tokenizer)\n",
    "\n",
    "#分词测试\n",
    "inputs = tokenizer.batch_encode_plus(\n",
    "    [[\n",
    "        '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间',\n",
    "        '的', '海', '域', '。'\n",
    "    ],\n",
    "     [\n",
    "         '这', '座', '依', '山', '傍', '水', '的', '博', '物', '馆', '由', '国', '内', '一',\n",
    "         '流', '的', '设', '计', '师', '主', '持', '设', '计', '，', '整', '个', '建', '筑',\n",
    "         '群', '精', '美', '而', '恢', '宏', '。'\n",
    "     ]],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "print(inputs.keys())\n",
    "for i in inputs:\n",
    "    print(i,inputs[i].shape)\n",
    "pretrained(**inputs).last_hidden_state.shape\n",
    "#del out3['token_embeddings'],out3['sentence_embedding']\n",
    "pretrained(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c10baa99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e312d40b1f24c50a181a1929adb2118",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/19.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "882f6abb6f4543689b4c86f458318365",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fce0e38677f4a5288c925dee5654728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/110k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29dff0135e1b4f08af57f518e09e6a27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/269k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a46a45d3fbae4287a711354653128784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)in/added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e463aa641084dbaa99d65455a016116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a4618b52a75459483a7052841c37d92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.08k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a38f305865b4d56884ea8c92d54dea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/505M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at thunlp/Lawformer were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of LongformerModel were not initialized from the model checkpoint at thunlp/Lawformer and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertTokenizerFast(name_or_path='DPR', vocab_size=23283, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "input_ids torch.Size([2, 37])\n",
      "token_type_ids torch.Size([2, 37])\n",
      "attention_mask torch.Size([2, 37])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 768])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2 = AutoTokenizer.from_pretrained(\"hfl/chinese-roberta-wwm-ext\")\n",
    "lawformer = AutoModel.from_pretrained(\"thunlp/Lawformer\")\n",
    "print(tokenizer)\n",
    "\n",
    "#分词测试\n",
    "inputs = tokenizer2.batch_encode_plus(\n",
    "    [[\n",
    "        '海', '钓', '比', '赛', '地', '点', '在', '厦', '门', '与', '金', '门', '之', '间',\n",
    "        '的', '海', '域', '。'\n",
    "    ],\n",
    "     [\n",
    "         '这', '座', '依', '山', '傍', '水', '的', '博', '物', '馆', '由', '国', '内', '一',\n",
    "         '流', '的', '设', '计', '师', '主', '持', '设', '计', '，', '整', '个', '建', '筑',\n",
    "         '群', '精', '美', '而', '恢', '宏', '。'\n",
    "     ]],\n",
    "    truncation=True,\n",
    "    padding=True,\n",
    "    return_tensors='pt',\n",
    "    is_split_into_words=True)\n",
    "\n",
    "print(inputs.keys())\n",
    "for i in inputs:\n",
    "    print(i,inputs[i].shape)\n",
    "lawformer(**inputs).last_hidden_state.shape\n",
    "#del out3['token_embeddings'],out3['sentence_embedding']\n",
    "lawformer(**inputs).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f43bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[32m██████████████████████████████████████████████████████████████████████████████\u001b[0m| 3000/3000 [00:40<00:00, 73.62it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in epoch 0 current train loss is 1.2100224494934082\n",
      "accuracy is  tensor([0.3393], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'model/dpr_微调.model'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-4831a1687559>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    150\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlf2classify\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model/dpr_微调.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlf2classify\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4831a1687559>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(epochs, device)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"accuracy is \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorrect\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlf2classify\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'model/dpr_微调.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    374\u001b[0m     \u001b[0m_check_dill_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpickle_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_use_new_zipfile_serialization\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'model/dpr_微调.model'"
     ]
    }
   ],
   "source": [
    "dpr=pretrained\n",
    "import torch\n",
    "import json\n",
    "class CailDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path):\n",
    "        self.path = path \n",
    "        file = open(path,'r',encoding='utf-8')\n",
    "        cases = file.readlines()\n",
    "        self.cases = [json.loads(i) for i in cases]\n",
    "        self.length = len(self.cases)\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.cases[index]\n",
    "        caseA = \"\".join(data['Case_A'])\n",
    "        caseB = \"\".join(data['Case_B'])\n",
    "        label = data['label']\n",
    "        inputs = tokenizer([caseA,caseB], return_tensors=\"pt\",padding=\"max_length\",max_length=300,truncation=True)\n",
    "        return inputs,label\n",
    "\n",
    "def collate_fn(data):\n",
    "    tensors = [i[0] for i in data]\n",
    "    labels = [i[1]  for i in data]\n",
    "    inputs_ids = torch.cat([i['input_ids'] for i in tensors])\n",
    "    token_type_ids = torch.cat([i['token_type_ids'] for i in tensors])\n",
    "    mask = torch.cat([i['attention_mask'] for i in tensors])\n",
    "    inputs = {\"input_ids\":inputs_ids,\"token_type_ids\":token_type_ids,\"attention_mask\":mask}\n",
    "    return inputs,labels\n",
    "\n",
    "trainset = CailDataset('./cail/competition_stage_2_train.json')\n",
    "#数据加载器\n",
    "trainloader = torch.utils.data.DataLoader(dataset=trainset,\n",
    "                                     batch_size=1,\n",
    "                                     collate_fn = collate_fn,\n",
    "                                     shuffle=True,\n",
    "                                     drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "#模型试算\n",
    "#[b, lens] -> [b, lens, 768]\n",
    "#pretrained(**inputs).last_hidden_state.shape\n",
    "import torch  \n",
    "class Lawfomer2Super(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tuneing = False\n",
    "        self.pretrained = dpr\n",
    "        # 300 is max length of lawformer \n",
    "        self.conv1d = torch.nn.Conv1d(in_channels=300,out_channels=150,kernel_size=1)\n",
    "        self.conv2d=torch.nn.Conv1d(in_channels=150,out_channels=1,kernel_size=1)\n",
    "        self.conv3d  = torch.nn.Conv2d(in_channels = 1, out_channels = 1,kernel_size=1)\n",
    "        self.fc = torch.nn.Linear(768, 3)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        #print(inputs.keys())\n",
    "        # inputs : batch_size * max_length * 768 \n",
    "        # conv1d output : batch_size * 1 * 768\n",
    "        # result : batch_size*1 * 3 \n",
    "        if self.tuneing:\n",
    "            out = self.pretrained(**inputs).last_hidden_state\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                out = self.pretrained(**inputs).last_hidden_state\n",
    "        #print(\"initial shape:\",out.shape)\n",
    "        \n",
    "        out = self.conv1d(out)\n",
    "        #print(\"After conv1d:\",out.shape)\n",
    "        out=self.conv2d(out)\n",
    "        #print(\"After conv2d:\",out.shape)\n",
    "        out = out.squeeze()\n",
    "        #print(\"After 1 squeeze:\",out.shape)\n",
    "        out = out.unsqueeze(0)\n",
    "        out=out.unsqueeze(0)\n",
    "        #print(\"After 2 unsqueezes:\",out.shape)\n",
    "        out = self.conv3d(out)\n",
    "        #print(\"After conv2d:\",out.shape)\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        out = out.squeeze()\n",
    "        out=torch.abs(out[0]-out[1]) #这里把out的两个batch的对应位置相减再取绝对值，是为了尽量突出差距，但效果不佳。\n",
    "        \n",
    "        out=torch.tensor([list(out)])  #match crossentropyloss\n",
    "        #print(\"Final:\",out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def fine_tuneing(self, tuneing):\n",
    "        self.tuneing = tuneing\n",
    "        if tuneing:\n",
    "            for i in self.pretrained.parameters():\n",
    "                i.requires_grad = True\n",
    "\n",
    "            lawformer.train()\n",
    "            self.pretrained =dpr\n",
    "        else:\n",
    "            for i in dpr.parameters():\n",
    "                i.requires_grad_(False)\n",
    "\n",
    "            lawformer.eval()\n",
    "            #print(\"any\")\n",
    "            self.pretrained = None\n",
    "\n",
    "lf2classify = Lawfomer2Super()\n",
    "lf2classify.fine_tuneing(tuneing=True)\n",
    "device = \"cuda:0\"\n",
    "lf2classify.to(device=device)\n",
    "lf2classify.pretrained.to(device)\n",
    "from transformers import AdamW\n",
    "#训练\n",
    "from tqdm import tqdm\n",
    "def train(epochs,device):\n",
    "    lr = 2e-4 if lf2classify.tuneing else 5e-4\n",
    "\n",
    "    #训练\n",
    "    optimizer = AdamW(lf2classify.parameters(), lr=lr)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    lf2classify.train()\n",
    "    for epoch in range(epochs):\n",
    "        lf2classify.train()\n",
    "        step = 0\n",
    "        correct = 0\n",
    "        for step, (inputs,labels) in tqdm(enumerate(trainloader),total=len(trainloader),colour='green'):\n",
    "            #print(\"labels\",labels)\n",
    "            step+=1\n",
    "            #模型计算\n",
    "            for i in inputs:\n",
    "                inputs[i] = inputs[i].to(device)\n",
    "            outs = lf2classify(inputs)\n",
    "            outs=outs.to(device)\n",
    "            #梯度下降\n",
    "            #outs = outs.squeeze()\n",
    "            labels = torch.tensor(labels).long()\n",
    "            #labels=torch.tensor([list(labels)])\n",
    "            labels = labels.to(device)\n",
    "            #print(\"cretirion\",outs,labels)\n",
    "            loss = criterion(outs,labels)\n",
    "            loss.requires_grad_(True)  #loss requires grad\n",
    "            fake = torch.argmax(outs)\n",
    "            \n",
    "            correct += fake==labels\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "        print(\"in epoch \"+str(epoch)+\" current train loss is \"+str(loss.item()))\n",
    "        print(\"accuracy is \",correct/len(trainset.cases))\n",
    "\n",
    "        torch.save(lf2classify, 'model/dpr_微调.model')\n",
    "\n",
    "train(5000,device)\n",
    "print(sum(p.numel() for p in lf2classify.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c1ef1531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.2793), tensor([[ 0.0020, -0.0980,  0.3000]]), tensor([1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c=torch.nn.CrossEntropyLoss()\n",
    "x=torch.tensor([0.002,-0.098,0.300])\n",
    "x=torch.tensor([list(x)])\n",
    "y=torch.tensor([1])\n",
    "c(x,y),x,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3140d05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0100, 0.0100, 0.0200])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "b=torch.tensor([0.01,0.02,0.03])\n",
    "c=torch.tensor([0.02,0.03,0.01])\n",
    "a=torch.abs(b-c)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018fd932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
