# work schedule 

从目前到交稿截至时间，还有两个月。 可以分为两个阶段，开学前，开学后。 

我们现在需要完成的工作有：

1. 分析预处理判决文书的数据
2. 构建模型，寻找方法(试错)
3. 搭建可视化系统（网页） 
4. 理论依据和论文写作 

前三项属于属于代码部分，最后一项是法律部分。 



在前一阶段较短的时间里，我们尽量完成主体部分工作。 开学后的时间进行收尾和美化。 

构建模型是我们的主要的逻辑的部分，但可视化系统的工作量也不小。 

## 开学前的二十天

1. 将数据集拆分到可以使用，对接DataLoader——why  大概两到三天

2. 开会确定一下模型输出的数据结构： 暂定2.1 你们先写大作业 WHY 处理数据 

   开会内容：

   1. 了解问题逻辑: 虚假诉讼 包括在事实层面造假的/在目的层面有损害第三方利益的意图的等等

   1. 确定主题和方向 ， 再给我们的研究取个名字啥的

      

   2. 划归问题：已有的数据及结构; 无监督的文本的表征-> 无监督学习分类  / 实体识别 关系抽取 

      >  对于法律概念实体进行识别，对于相同词组的不同表达进行映射
      >
      > 可以借鉴医学领域的术语识别的一些思路
      >
      > 然后展示实体识别的结果
      >
      > 这样我们可以对案情进行相对准确的建模 
      >
      > 
      >
      > 然后可以对当事人，审判员，法院，这些主体进行建模，抽取建立每个案情的关系，便利人工审查 
      >
      > 此处需要 对 人名 地名 进行识别
      >
      > 人名地名之间的关系应该用法律术语来表示 也就是上一步的子集
      >
      > 
      >
      > 下一步可以做案情相似度的分析，或许计算两个案情图之间的相似度（这里应该是带有一点拓扑的意思？）
      >
      > 然后我们去做classification 做分类预测 

      

   1. 筛选模型：BERT: 微调和预训练  / others etc.  

   2. 可视化的方向  

      > 搜索系统  和 可视化的统计系统 
      >
      > 这个页面理论上应该是可以拓展的 即 页面和数据分离  
      >
      > 搜索系统已有参考
      >
      > 可视化系统
      >
      > 除了统计上的描述性统计图之外
      >
      > 还可以做类似沙盘之类的标注图，便于快速了解总体状况 

      

      

   会议记录：

   > 1. 虚假诉讼 
   >
   > 数据问题： 每个案子一个字典  -> 抽出特征 ->  
   >
   > 借助法律知识， 人工提取特征，构建正例和负例 , bert 编码 ， 小规模学习 
   >
   > 划分小类 
   >
   > classification or unsupervised learning  
   >
   > 维度1： 单个案子，纵深的方向挖掘特征  
   >
   > 方案一： 人工抽取特征 　－>  劳动力密集型 
   >
   > 方案二： end -> end 无监督学习  最主体的部分： embedding (基于自然语言文本) ->R^d  -> 计算相似度 
   >
   > 首先： 虚假诉讼 样本比例 悬殊， 也缺乏合适标签。  
   >
   > 再在空间里面进行标注虚假诉讼的例子 -> 可能会映射成几簇  对应着 不同的虚假诉讼的类型
   >
   > 我们在推断-> 投票 （离input最近的几个案子里面，有多少个虚假诉讼案子，的比例）  
   >
   > ​	改进： 考虑到 文本的组织 并不规范？ 一致， （存在 同一个意思对应多个词语）  -> 实体识别，将案子映射到无关语言组织方式，只和情节有关的高位空间-> 提高embedding 效率，增加可解释性， 提高预测效率 
   >
   > 不同文本出发-》无关文本的事件模型->1个embeeding  
   >
   > 怎么映射文本和事件： 文本提取案件结构（生成故事梗概） 
   >
   > A->B  B->A  B-> C  
   >
   > A’->b, b->A’,
   >
   > 实体识别（1. 抽出P,L,O masked , 2. 法律概念： 帮助进行关系抽取，建立 不同表达到相同概念的映射 ），句法识别（抽取谓语） ，
   >
   > 优点： 可解释性好-》 好讲故事，好写论文，并且高度模拟人的工作 ，中间结果是可以可视化 
   >
   > 缺点： 比较困难(有新方法 )， 不一定work 
   >
   > ​	数据增强: （存在 同一个意思对应多个词语） -> generate 虚假诉讼的案例  -> chatgpt text-> 一个案子的第二个文本表达->多了按一个案子->虚假诉讼案子数量 
   >
   > BERT_base_chinese -> pretrain fine-tuning -> 喂 民法典  的  
   >
   > 文本生成器： chatgpt -> 同义改写任务 -》 增加虚假诉讼案例  
   >
   > 优点： 不用我们自己写太多代码，把别人的拿来用 
   >
   > 缺点是： 他可能只能输出结果    
   >
   > 
   >
   > 同一个事件-》不同的文本-> 不同embeeding -> 分为同一类 
   >
   > 在人名地名都mask掉，我们只看 案件结构
   >
   >  
   >
   > 数学公式推导（损失函数） 
   >
   > 
   >
   > 维度2： 不同案子横向挖掘联系 
   >
   > 考虑人名地名，法院组织名  
   >
   > 不需要深度学习了？  
   >
   > 高效的数据库： 给出 同一个当事人 的不同案件-》 提取关联案件 
   >
   > ...机器学习，统计  

   [lansinuote/NER_in_Chinese (github.com)](https://github.com/lansinuote/NER_in_Chinese)

   

   > bk ： transformer multi head self-attention  mechanism rnn
   >
   > bert: tranformer 的编码器  ， 抹掉文本和预测句子关系两个预训练任务 -> 微调
   >
   > gpt: transformer的解码器， masked 预测，
   >
   > gpt2: 预训练之后，zero-shot 直接预测输出 
   >
   > gpt3更大 ，不需要微调,few-shot 
   >
   > task: 了解原理，熟悉公式，然后 bert用来encode , gpt用来生成 虚假的虚假诉讼 
   >
   > 先跑通预训练模型  

   > why: 
   >
   > 案子的横向联系  
   >
   > bert -》  实体识别关系抽取什么的的， 句法识别，知识图谱

   

   可视化内容:

   1. 首先是 deduction module  :  input text output 分析报告 

      text->支持一下文本复制站体 和上传本地文件  （大规模推断） 

      分析报告： 概率（相似度） ，相似案件（连接） ，本案件的结构（图），具体的虚假类型，可能危害的第三方

      参考： 查重报告？

      2.   总体情况： 虚假诉讼的空间分布， 聚类信息-》 不同的类， 然后 高发案情警告 （让一个不太懂的人点进来，能快速了解虚假诉讼） 

      

      建议是： 每日更新进度，包括遇到的困难，新的想法  

      第二次会议记录 2-5
      
      > 在案件内部，进行一个切分： 起因，经过，结果。 
      >
      > 虚假诉讼： 用有误导性的证据 引导法院做出可能对第三方利益有损害的判决 
      >
      > 情况一： 有一方提供了假证据，然后但是法院没有支持他的请求，那么他就构成了虚假诉讼动机，但是没有造成第三方利益损害单价结果。   
      >
      > 情况二： 双方都没有提供假证据，然后通过串通，让法院判了一个判决。 导致他们逃避了第三方的债务。 （还债是有优先级） 导致损害了第三方利益 
      >
      > 情况三： 职业借贷人。  伪造证据。  学他的牵扯的案子。 （横向的处理） 
      >
      > 
      >
      > 所以 判决的结果重要性可能相对较高（猜想）  
      >
      > 从经过的描述文本里面学习可能是虚假诉讼的概率—— 这个证据是假证据的概率， 这个是没办法学的。 
      >
      > 从结果的描述文本学习可能是虚假诉讼的概率—— 学习 一个判决对第三方利益可能有损害的概率（配没赔钱，赔了多少，佩给谁了（老公，亲人，朋友）  ， 牵扯的别的纠纷）  
      >
      > ——》 从案子里面 截取 判决 -》 潜在好处之一： embedding的sample 的长度 更整齐了。 
      
      
      
      > 先对文本预处理，
      
      
      
      
      
      > 三个自然语言模型：
      >
      > 利用预训练模型对句子做编码 —— sentenceTransformer 
      >
      > 做一个实体识别关系抽取 —— 建模不同案件的相似度  一方面考虑关系 一个考虑主题
      >
      > 
      >
      > 数据清洗问题：  
      >
      > 下游数据集： semi-supervised  20000 (NER 20000<)
      >
      >    1. 实体识别：  法律术语， 从 案件抽取来的关键词（用案子as input, 分词，抽取TFIDF高的关键词) ，我们再手工对关键词进行分类，分类再去标注 案件文本（比较自动） ->  学习识别实体 
      >
      >       
      >
      > 模型定义： cnn rnn  attention block 
      >
      > 序列标注和关系分类： 
      >
      > 关系分类 ： 定义关系分类 ->  (entity-relation,entity)
      >
      > 序列标注： 输出的结果是
      >
      > 实体识别： 
      
      
      
      
      
      

   

3. 并行可视化和模型 

   3.1 特征工程 模型训练 模型预测 模型修改 调优

   > 这一部分的工作的特点是： 工作模式确定了，但是在模型性能上会有一些无法预料的困难

   

   3.2 可视化系统= 搜索引擎+ 依据地图建模的动图

   > 这一部分的工作的特点是： 暂时学习路径不明，时间会主要花费在学什么上。 至于具体的coding应该不会太难

   3.3 初步论文写作

   motivation 、 significance , theory evidence 

4. 合并模型，进行试运行

这一步大概两到三天

## 开学后的四十天